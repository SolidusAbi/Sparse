{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LinearL0(nn.Linear):\n",
    "    r'''\n",
    "        Linear layer with L0 norm regularization.\n",
    "\n",
    "        Paper: \n",
    "            Louizos, C., Welling, M., & Kingma, D. P. (2017). \n",
    "            Learning sparse neural networks through $ L_0 $ regularization. \n",
    "            arXiv preprint arXiv:1712.01312.\n",
    "\n",
    "        Class highly inspirired by the original implementation:\n",
    "            https://github.com/AMLab-Amsterdam/L0_regularization/blob/master/l0_layers.py\n",
    "\n",
    "        Args:\n",
    "            in_features: number of features in the input tensor\n",
    "            out_features: number of features in the output tensor\n",
    "            bias: If set to False, the layer will not learn an additive bias.\n",
    "                Default: True\n",
    "            weight_decay: weight decay for L2 regularization\n",
    "            droprate_init: dropout rate for the initial dropout mask, the weight gates\n",
    "            temperature: temperature for the Concrete Distribution gate\n",
    "            lamba: lambda for the L0 regularization\n",
    "            local_rep: Whether we will use a separate gate sample per element in the minibatch\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bias=True, weight_decay=1., droprate_init=0.5, temperature=.1,\n",
    "                 lamba=1., local_rep=False, **kwargs) -> None:\n",
    "        super(LinearL0, self).__init__(in_features, out_features, bias=bias, **kwargs)\n",
    "        \n",
    "        logit_drp_init = math.log(1 - droprate_init) - math.log(droprate_init)\n",
    "        self.qz_loga = nn.Parameter(torch.FloatTensor(in_features).normal_(logit_drp_init, .2))\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lambda_ = lamba\n",
    "        self.temperature = temperature\n",
    "        self.local_rep = local_rep\n",
    "        \n",
    "        # Attributes for Concrete Distribution\n",
    "        self.limit_a, self.limit_b, self.eps = -.1, 1.1, 1e-6\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.local_rep or not self.training:\n",
    "            z = self.sample_z(x.size(0), sample=self.training)\n",
    "            xin = x.mul(z)\n",
    "            output = F.linear(xin, self.weight, self.bias)\n",
    "        else:\n",
    "            weights = self.sample_weights()\n",
    "            output = F.linear(x, weights, self.bias)\n",
    "        return output\n",
    "\n",
    "    def constrain_parameters(self, **kwargs):\n",
    "        self.qz_loga.data.clamp_(min=math.log(1e-2), max=math.log(1e2))\n",
    "\n",
    "    def sample_z(self, batch_size, sample=True):\n",
    "        r'''\n",
    "            Sample the hard-concrete gates for training and use a deterministic value for testing\n",
    "        '''\n",
    "        if sample:\n",
    "            float_tensor = torch.cuda.FloatTensor if self.qz_loga.is_cuda else torch.FloatTensor\n",
    "            eps = self.get_eps(float_tensor(batch_size, self.in_features))\n",
    "            z = self.quantile_concrete(eps)\n",
    "            return F.hardtanh(z, min_val=0, max_val=1)\n",
    "        else:  # mode\n",
    "            pi = torch.sigmoid(self.qz_loga).view(1, self.in_features).expand(batch_size, self.in_features)\n",
    "            return F.hardtanh(pi * (self.limit_b - self.limit_a) + self.limit_a, min_val=0, max_val=1)\n",
    "\n",
    "    def sample_weights(self):\n",
    "        z = self.quantile_concrete(self.get_eps(self.in_features))\n",
    "        mask = F.hardtanh(z, min_val=0, max_val=1) # Hard concrete gate\n",
    "        return mask.view(1, self.in_features) * self.weight\n",
    "\n",
    "    def get_eps(self, size):\n",
    "        r'''\n",
    "            Uniform random numbers for the concrete distribution\n",
    "        '''\n",
    "        float_tensor = torch.cuda.FloatTensor if self.qz_loga.is_cuda else torch.FloatTensor\n",
    "        eps = Variable(float_tensor(size).uniform_(self.eps, 1-self.eps))\n",
    "        return eps\n",
    "        \n",
    "    def quantile_concrete(self, x):\n",
    "        r'''\n",
    "            Implements the quantile, aka inverse CDF, \n",
    "            of the 'stretched' concrete distribution\n",
    "        '''\n",
    "        y = torch.sigmoid((torch.log(x) - torch.log(1 - x) + self.qz_loga) / self.temperature)\n",
    "        return y * (self.limit_b - self.limit_a) + self.limit_a\n",
    "\n",
    "    def cdf_qz(self, x):\n",
    "        r'''\n",
    "            Implements the CDF of the 'stretched' concrete distribution\n",
    "        '''\n",
    "        xn = (x - self.limit_a) / (self.limit_b - self.limit_a)\n",
    "        logits = math.log(xn) - math.log(1 - xn)\n",
    "        return torch.sigmoid(logits * self.temperature - self.qz_loga).clamp(min=self.eps, max=1 - self.eps)\n",
    "\n",
    "    def _reg_w(self):\n",
    "        r'''\n",
    "            Expected L0 norm under the stochastic gates, takes into account \n",
    "            and re-weights also a potential L2 penalty\n",
    "        '''\n",
    "        logpw_col = torch.sum(- (.5 * self.weight_decay * self.weight.pow(2)) - self.lambda_, 0)\n",
    "        logpw = torch.sum((1 - self.cdf_qz(0)) * logpw_col)\n",
    "        logpb = - torch.sum(.5 * self.weight_decay * self.bias.pow(2)) if self.bias is not None else 0\n",
    "        return logpw + logpb\n",
    "\n",
    "    def regularization(self):\n",
    "        return self._reg_w()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BreastCancer(Dataset):\n",
    "    r'''\n",
    "        Breast Cancer Wisconsin Dataset\n",
    "    '''\n",
    "    def __init__(self, normalize=False):\n",
    "        dataset = datasets.load_breast_cancer()\n",
    "        self.data = torch.tensor(dataset.data).float()\n",
    "        self.targets = torch.tensor(dataset.target)\n",
    "    \n",
    "        if normalize:\n",
    "            self.data /= torch.max(self.data, dim=0)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "               LinearL0(input_size, hidden_size, bias=True, weight_decay=1, droprate_init=.5, lamba=1, temperature=.1),\n",
    "               nn.ReLU(inplace=True),\n",
    "               LinearL0(hidden_size, hidden_size//2, bias=True, weight_decay=1, droprate_init=.5, lamba=-1e-5, temperature=.1),\n",
    "               nn.ReLU(inplace=True),\n",
    "               LinearL0(hidden_size//2, output_size, bias=True, weight_decay=1, droprate_init=.5, lamba=-1e-5, temperature=.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def regularization(self, reg_factor = 1e-4):\n",
    "        reg = 0.\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, LinearL0):\n",
    "                reg = reg  + (-reg_factor * module.regularization())\n",
    "        return reg\n",
    "\n",
    "    def constrain_parameters(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, LinearL0):\n",
    "                module.constrain_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataset, batch_size = 128, n_epochs=10):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1},\n",
    "        )\n",
    "\n",
    "    for _ in epoch_iterator:\n",
    "        for idx, (inputs, targets) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pred = model(inputs)\n",
    "\n",
    "            loss = criterion(pred, targets) + model.regularization()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clamp the parameters\n",
    "            model.constrain_parameters()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BreastCancer(normalize=True)\n",
    "\n",
    "eval_len = len(dataset) // 5 # 20% of the dataset\n",
    "train_set, eval_set = torch.utils.data.random_split(dataset, [len(dataset) - eval_len, eval_len])\n",
    "\n",
    "loader = DataLoader(eval_set, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(30, 48, 2)\n",
    "train(model, train_set, batch_size=128, n_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loader))\n",
    "model.eval()\n",
    "result = torch.argmax(model(x.cuda()),axis=1)\n",
    "print('Accuracy:', (result == y.cuda()).sum().item() / len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sigmoid(model.model[0].qz_loga.detach()).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = model.model[0]\n",
    "weight = first_layer.sample_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(1000):\n",
    "    samples.append(first_layer.sample_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.stack(samples).detach()\n",
    "\n",
    "test = test.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "values = test.cpu().numpy()\n",
    "for i in np.linspace(0, 29, 6, dtype=int):\n",
    "    plt.plot(values[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Ranking\n",
    "Muestras con alta varianza las considero importantes! Parece que coincide con los valores esperados de otras pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loader))\n",
    "first_layer = model.model[0]\n",
    "\n",
    "samples = []\n",
    "for i in range(1000):\n",
    "    samples.append(first_layer.sample_weights())\n",
    "\n",
    "values = torch.stack(samples).detach().mean(axis=0)\n",
    "std_values = values.std(axis=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, idx = torch.sort(std_values)\n",
    "features_names = datasets.load_breast_cancer(as_frame=True).data.columns[idx.flipud().cpu()]\n",
    "print(features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('HySpecLab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc81a3ec444beb1d5a523daf231afa571e79be8a57abb6fe0028623a3d4d7136"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
