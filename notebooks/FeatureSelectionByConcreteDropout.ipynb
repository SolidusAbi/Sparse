{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "from Sparse.modules.variational import LinearCD\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class LinearCD(nn.Linear):\n",
    "    r'''\n",
    "        Linear layer with Concrete Dropout regularization.\n",
    "\n",
    "        Code strongly inspired by: \n",
    "            https://github.com/danielkelshaw/ConcreteDropout/blob/master/condrop/concrete_dropout.py\n",
    "\n",
    "        Note the relationship between the weight regularizer (w_reg) and dropout regularization (drop_reg):\n",
    "        \n",
    "            w_reg/drop_reg = (l^2)/2 \n",
    "        \n",
    "        with prior lengthscale l (number of in_features). \n",
    "        \n",
    "        Note also that the factor of two should be ignored for cross-entropy loss, and used only for the\n",
    "        Euclidean loss.\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bias=True, threshold=.95, init_min=0.05, init_max=0.1):\n",
    "        super(LinearCD, self).__init__(in_features, out_features, bias)        \n",
    "        logit_init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        logit_init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "        \n",
    "        # The probability of deactive a neuron.\n",
    "        self.logit_p = nn.Parameter(torch.rand(in_features) * (logit_init_max - logit_init_min) + logit_init_min)\n",
    "        self.logit_threshold = np.log(threshold) - np.log(1. - threshold)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return F.linear(self.concrete_bernoulli(x), self.weight, self.bias)\n",
    "\n",
    "        return F.linear(x, self.weight * (self.logit_p < self.logit_threshold).float(), self.bias) \n",
    "\n",
    "    def concrete_bernoulli(self, x):\n",
    "        eps = 1e-8\n",
    "        unif_noise = torch.cuda.FloatTensor(*x.size()).uniform_() if self.logit_p.is_cuda else torch.FloatTensor(*x.size()).uniform_()\n",
    "\n",
    "        p = torch.sigmoid(self.logit_p)\n",
    "        tmp = .1\n",
    "\n",
    "        drop_prob = (torch.log(p + eps) - torch.log((1-p) + eps) + torch.log(unif_noise + eps)\n",
    "        - torch.log((1. - unif_noise) + eps))\n",
    "        drop_prob = torch.sigmoid(drop_prob / tmp)\n",
    "\n",
    "        random_tensor = 1 - drop_prob\n",
    "        retain_prob = 1 - p # rescale factor typical for dropout\n",
    "\n",
    "        if self.training:\n",
    "            self.activation_reg = random_tensor.sum(dim=1).mean() # Penalizing the number of features activated\n",
    "\n",
    "        return torch.mul(x, random_tensor) / retain_prob\n",
    "\n",
    "    def reg(self):\n",
    "        return self.activation_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Wisconsin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BreastCancer(Dataset):\n",
    "    r'''\n",
    "        Breast Cancer Wisconsin Dataset\n",
    "    '''\n",
    "    def __init__(self, normalize=False):\n",
    "        dataset = datasets.load_breast_cancer()\n",
    "        self.data = torch.tensor(dataset.data).float()\n",
    "        self.targets = torch.tensor(dataset.target)\n",
    "    \n",
    "        if normalize:\n",
    "            self.data /= torch.max(self.data, dim=0)[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, nb_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            LinearCD(30, nb_features, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features, nb_features//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_features//2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BreastCancer(normalize=True)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataset, batch_size = 64, n_epochs=10):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    kl_reg = 1e-6\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epochs),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1},\n",
    "        )\n",
    "\n",
    "    for _ in epoch_iterator:\n",
    "        kl_reg = min(kl_reg + .1e-2, 1e-1)\n",
    "        for idx, (inputs, targets) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pred = model(inputs)\n",
    "\n",
    "            loss = criterion(pred, targets) + 1e-1*model.model[0].reg()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:18<00:00, 32.40epoch/s, tls=0.8433]\n"
     ]
    }
   ],
   "source": [
    "model = Model(512)\n",
    "model = train(model, dataset, n_epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False,  True, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False,  True, False, False],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(model.model[0].logit_p) < .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.7976, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model[0].reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8297, 0.7290, 0.8158, 0.8406, 0.7978, 0.8826, 0.7381, 0.1367, 0.7901,\n",
       "        0.1992, 0.7448, 0.8966, 0.8553, 0.8460, 0.8912, 0.8829, 0.8925, 0.8951,\n",
       "        0.8851, 0.8261, 0.8206, 0.4524, 0.8093, 0.7189, 0.7741, 0.8110, 0.7671,\n",
       "        0.1566, 0.7976, 0.8744], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(model.model[0].logit_p) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('HySpecLab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc81a3ec444beb1d5a523daf231afa571e79be8a57abb6fe0028623a3d4d7136"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
