{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def maxEntropy(n,k):\n",
    "  \"\"\"\n",
    "  The maximum enropy we could get with n units and k winners\n",
    "  \"\"\"\n",
    "\n",
    "  s = float(k)/n\n",
    "  if s > 0.0 and s < 1.0:\n",
    "    entropy = - s * math.log(s,2) - (1 - s) * math.log(1 - s,2)\n",
    "  else:\n",
    "    entropy = 0\n",
    "\n",
    "  return n*entropy\n",
    "\n",
    "\n",
    "def binaryEntropy(x):\n",
    "  \"\"\"\n",
    "  Calculate entropy for a list of binary random variables\n",
    "  :param x: (torch tensor) the probability of the variable to be 1.\n",
    "  :return: entropy: (torch tensor) entropy, sum(entropy)\n",
    "  \"\"\"\n",
    "  entropy = - x*x.log2() - (1-x)*(1-x).log2()\n",
    "  entropy[x*(1 - x) == 0] = 0\n",
    "  return entropy, entropy.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Numenta Platform for Intelligent Computing (NuPIC)\n",
    "# Copyright (C) 2018, Numenta, Inc.  Unless you have an agreement\n",
    "# with Numenta, Inc., for a separate license for this software code, the\n",
    "# following terms and conditions apply:\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero Public License version 3 as\n",
    "# published by the Free Software Foundation.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
    "# See the GNU Affero Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero Public License\n",
    "# along with this program.  If not, see http://www.gnu.org/licenses.\n",
    "#\n",
    "# http://numenta.org/licenses/\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class k_winners(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  A simple K-winner take all autograd function for creating layers with sparse\n",
    "  output.\n",
    "   .. note::\n",
    "      Code adapted from this excellent tutorial:\n",
    "      https://github.com/jcjohnson/pytorch-examples\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(ctx, x, dutyCycles, k, boostStrength):\n",
    "    \"\"\"\n",
    "    Use the boost strength to compute a boost factor for each unit represented\n",
    "    in x. These factors are used to increase the impact of each unit to improve\n",
    "    their chances of being chosen. This encourages participation of more columns\n",
    "    in the learning process.\n",
    "    The boosting function is a curve defined as: boostFactors = exp[ -\n",
    "    boostStrength * (dutyCycle - targetDensity)] Intuitively this means that\n",
    "    units that have been active (i.e. in the top-k) at the target activation\n",
    "    level have a boost factor of 1, meaning their activity is not boosted.\n",
    "    Columns whose duty cycle drops too much below that of their neighbors are\n",
    "    boosted depending on how infrequently they have been active. Unit that has\n",
    "    been active more than the target activation level have a boost factor below\n",
    "    1, meaning their activity is suppressed and they are less likely to be in \n",
    "    the top-k.\n",
    "    Note that we do not transmit the boosted values. We only use boosting to\n",
    "    determine the winning units.\n",
    "    The target activation density for each unit is k / number of units. The\n",
    "    boostFactor depends on the dutyCycle via an exponential function:\n",
    "            boostFactor\n",
    "                ^\n",
    "                |\n",
    "                |\\\n",
    "                | \\\n",
    "          1  _  |  \\\n",
    "                |    _\n",
    "                |      _ _\n",
    "                |          _ _ _ _\n",
    "                +--------------------> dutyCycle\n",
    "                   |\n",
    "              targetDensity\n",
    "    :param ctx: \n",
    "      Place where we can store information we will need to compute the gradients\n",
    "      for the backward pass.\n",
    "    :param x: \n",
    "      Current activity of each unit.  \n",
    "    :param dutyCycles: \n",
    "      The averaged duty cycle of each unit.\n",
    "    :param k: \n",
    "      The activity of the top k units will be allowed to remain, the rest are\n",
    "      set to zero.\n",
    "                \n",
    "    :param boostStrength:     \n",
    "      A boost strength of 0.0 has no effect on x.\n",
    "    :return: \n",
    "      A tensor representing the activity of x after k-winner take all.\n",
    "    \"\"\"\n",
    "    if boostStrength > 0.0:\n",
    "      targetDensity = float(k) / x.size(1)\n",
    "      boostFactors = torch.exp((targetDensity - dutyCycles) * boostStrength)\n",
    "      boosted = x.detach() * boostFactors\n",
    "    else:\n",
    "      boosted = x.detach()\n",
    "\n",
    "    # Take the boosted version of the input x, find the top k winners.\n",
    "    # Compute an output that contains the values of x corresponding to the top k\n",
    "    # boosted values\n",
    "    res = torch.zeros_like(x)\n",
    "    topk, indices = boosted.topk(k, sorted=False)\n",
    "    for i in range(x.shape[0]):\n",
    "      res[i, indices[i]] = x[i, indices[i]]\n",
    "\n",
    "    ctx.save_for_backward(indices)\n",
    "    return res\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass, we set the gradient to 1 for the winning units, and 0\n",
    "    for the others.\n",
    "    \"\"\"\n",
    "    indices, = ctx.saved_tensors\n",
    "    grad_x = torch.zeros_like(grad_output, requires_grad=True)\n",
    "\n",
    "    # Probably a better way to do it, but this is not terrible as it only loops\n",
    "    # over the batch size.\n",
    "    for i in range(grad_output.size(0)):\n",
    "      grad_x[i, indices[i]] = grad_output[i, indices[i]]\n",
    "\n",
    "    return grad_x, None, None, None\n",
    "\n",
    "\n",
    "\n",
    "class k_winners2d(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  A K-winner take all autograd function for CNN 2D inputs (batch, Channel, H, W).\n",
    "  .. seealso::\n",
    "       Function :class:`k_winners`\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(ctx, x, dutyCycles, k, boostStrength):\n",
    "    \"\"\"\n",
    "    Use the boost strength to compute a boost factor for each unit represented\n",
    "    in x. These factors are used to increase the impact of each unit to improve\n",
    "    their chances of being chosen. This encourages participation of more columns\n",
    "    in the learning process. See :meth:`k_winners.forward` for more details.\n",
    "    :param ctx:\n",
    "      Place where we can store information we will need to compute the gradients\n",
    "      for the backward pass.\n",
    "    :param x:\n",
    "      Current activity of each unit.\n",
    "    :param dutyCycles:\n",
    "      The averaged duty cycle of each unit.\n",
    "    :param k:\n",
    "      The activity of the top k units will be allowed to remain, the rest are\n",
    "      set to zero.\n",
    "    :param boostStrength:\n",
    "      A boost strength of 0.0 has no effect on x.\n",
    "    :return:\n",
    "      A tensor representing the activity of x after k-winner take all.\n",
    "    \"\"\"\n",
    "    batchSize = x.shape[0]\n",
    "    if boostStrength > 0.0:\n",
    "      targetDensity = float(k) / (x.shape[1] * x.shape[2] * x.shape[3])\n",
    "      boostFactors = torch.exp((targetDensity - dutyCycles) * boostStrength)\n",
    "      boosted = x.detach() * boostFactors\n",
    "    else:\n",
    "      boosted = x.detach()\n",
    "\n",
    "    # Take the boosted version of the input x, find the top k winners.\n",
    "    # Compute an output that only contains the values of x corresponding to the top k\n",
    "    # boosted values. The rest of the elements in the output should be 0.\n",
    "    boosted = boosted.reshape((batchSize, -1))\n",
    "    xr = x.reshape((batchSize, -1))\n",
    "    res = torch.zeros_like(boosted)\n",
    "    topk, indices = boosted.topk(k, dim=1, sorted=False)\n",
    "    res.scatter_(1, indices, xr.gather(1, indices))\n",
    "    res = res.reshape(x.shape)\n",
    "\n",
    "    ctx.save_for_backward(indices)\n",
    "    return res\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass, we set the gradient to 1 for the winning units, and 0\n",
    "    for the others.\n",
    "    \"\"\"\n",
    "    batchSize = grad_output.shape[0]\n",
    "    indices, = ctx.saved_tensors\n",
    "\n",
    "    g = grad_output.reshape((batchSize, -1))\n",
    "    grad_x = torch.zeros_like(g, requires_grad=False)\n",
    "    grad_x.scatter_(1, indices, g.gather(1, indices))\n",
    "    grad_x = grad_x.reshape(grad_output.shape)\n",
    "\n",
    "    return grad_x, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Numenta Platform for Intelligent Computing (NuPIC)\n",
    "# Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement\n",
    "# with Numenta, Inc., for a separate license for this software code, the\n",
    "# following terms and conditions apply:\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero Public License version 3 as\n",
    "# published by the Free Software Foundation.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
    "# See the GNU Affero Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero Public License\n",
    "# along with this program.  If not, see http://www.gnu.org/licenses.\n",
    "#\n",
    "# http://numenta.org/licenses/\n",
    "# ----------------------------------------------------------------------\n",
    "from __future__ import print_function\n",
    "import abc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from pytorch.duty_cycle_metrics import (\n",
    "#   maxEntropy, binaryEntropy\n",
    "# )\n",
    "# from pytorch.functions import k_winners, k_winners2d\n",
    "\n",
    "\n",
    "\n",
    "def updateBoostStrength(m):\n",
    "  \"\"\"\n",
    "  Function used to update KWinner modules boost strength after each epoch.\n",
    "  Call using :meth:`torch.nn.Module.apply` after each epoch if required\n",
    "  For example: ``m.apply(updateBoostStrength)``\n",
    "  :param m: KWinner module\n",
    "  \"\"\"\n",
    "  if isinstance(m, KWinnersBase):\n",
    "    if m.training:\n",
    "      m.boostStrength = m.boostStrength * m.boostStrengthFactor\n",
    "\n",
    "\n",
    "\n",
    "class KWinnersBase(nn.Module):\n",
    "  \"\"\"\n",
    "  Base KWinners class\n",
    "  \"\"\"\n",
    "  __metaclass__ = abc.ABCMeta\n",
    "\n",
    "\n",
    "  def __init__(self, n, k, kInferenceFactor=1.0, boostStrength=1.0,\n",
    "               boostStrengthFactor=1.0, dutyCyclePeriod=1000):\n",
    "    \"\"\"\n",
    "    :param n:\n",
    "      Number of units\n",
    "    :type n: int\n",
    "    :param k:\n",
    "      The activity of the top k units will be allowed to remain, the rest are set\n",
    "      to zero\n",
    "    :type k: int\n",
    "    :param kInferenceFactor:\n",
    "      During inference (training=False) we increase k by this factor.\n",
    "    :type kInferenceFactor: float\n",
    "    :param boostStrength:\n",
    "      boost strength (0.0 implies no boosting).\n",
    "    :type boostStrength: float\n",
    "    :param boostStrengthFactor:\n",
    "      Boost strength factor to use [0..1]\n",
    "    :type boostStrengthFactor: float\n",
    "    :param dutyCyclePeriod:\n",
    "      The period used to calculate duty cycles\n",
    "    :type dutyCyclePeriod: int\n",
    "    \"\"\"\n",
    "    super(KWinnersBase, self).__init__()\n",
    "    assert (boostStrength >= 0.0)\n",
    "\n",
    "    self.n = n\n",
    "    self.k = k\n",
    "    self.kInferenceFactor = kInferenceFactor\n",
    "    self.learningIterations = 0\n",
    "\n",
    "    # Boosting related parameters\n",
    "    self.boostStrength = boostStrength\n",
    "    self.boostStrengthFactor = boostStrengthFactor\n",
    "    self.dutyCyclePeriod = dutyCyclePeriod\n",
    "\n",
    "\n",
    "  def getLearningIterations(self):\n",
    "    return self.learningIterations\n",
    "\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def updateDutyCycle(self, x):\n",
    "    \"\"\"\n",
    "     Updates our duty cycle estimates with the new value. Duty cycles are\n",
    "     updated according to the following formula:\n",
    "    .. math::\n",
    "        dutyCycle = \\\\frac{dutyCycle \\\\times \\\\left( period - batchSize \\\\right)\n",
    "                            + newValue}{period}\n",
    "    :param x:\n",
    "      Current activity of each unit\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "  def updateBoostStrength(self):\n",
    "    \"\"\"\n",
    "    Update boost strength using given strength factor during training\n",
    "    \"\"\"\n",
    "    if self.training:\n",
    "      self.boostStrength = self.boostStrength * self.boostStrengthFactor\n",
    "\n",
    "\n",
    "  def entropy(self):\n",
    "    \"\"\"\n",
    "    Returns the current total entropy of this layer\n",
    "    \"\"\"\n",
    "    if self.k < self.n:\n",
    "      _, entropy = binaryEntropy(self.dutyCycle)\n",
    "      return entropy\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "\n",
    "  def maxEntropy(self):\n",
    "    \"\"\"\n",
    "    Returns the maximum total entropy we can expect from this layer\n",
    "    \"\"\"\n",
    "    return maxEntropy(self.n, self.k)\n",
    "\n",
    "\n",
    "\n",
    "class KWinners(KWinnersBase):\n",
    "  \"\"\"\n",
    "  Applies K-Winner function to the input tensor\n",
    "  See :class:`htmresearch.frameworks.pytorch.functions.k_winners`\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, n, k, kInferenceFactor=1.0, boostStrength=1.0,\n",
    "               boostStrengthFactor=1.0, dutyCyclePeriod=1000):\n",
    "    \"\"\"\n",
    "    :param n:\n",
    "      Number of units\n",
    "    :type n: int\n",
    "    :param k:\n",
    "      The activity of the top k units will be allowed to remain, the rest are set\n",
    "      to zero\n",
    "    :type k: int\n",
    "    :param kInferenceFactor:\n",
    "      During inference (training=False) we increase k by this factor.\n",
    "    :type kInferenceFactor: float\n",
    "    :param boostStrength:\n",
    "      boost strength (0.0 implies no boosting).\n",
    "    :type boostStrength: float\n",
    "    :param boostStrengthFactor:\n",
    "      Boost strength factor to use [0..1]\n",
    "    :type boostStrengthFactor: float\n",
    "    :param dutyCyclePeriod:\n",
    "      The period used to calculate duty cycles\n",
    "    :type dutyCyclePeriod: int\n",
    "    \"\"\"\n",
    "\n",
    "    super(KWinners, self).__init__(n=n, k=k,\n",
    "                                   kInferenceFactor=kInferenceFactor,\n",
    "                                   boostStrength=boostStrength,\n",
    "                                   boostStrengthFactor=boostStrengthFactor,\n",
    "                                   dutyCyclePeriod=dutyCyclePeriod)\n",
    "    self.register_buffer(\"dutyCycle\", torch.zeros(self.n))\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Apply k-winner algorithm if k < n, otherwise default to standard RELU\n",
    "    if self.k >= self.n:\n",
    "      return F.relu(x)\n",
    "\n",
    "    if self.training:\n",
    "      k = self.k\n",
    "    else:\n",
    "      k = min(int(round(self.k * self.kInferenceFactor)), self.n)\n",
    "\n",
    "    x = k_winners.apply(x, self.dutyCycle, k, self.boostStrength)\n",
    "\n",
    "    if self.training:\n",
    "      self.updateDutyCycle(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "  def updateDutyCycle(self, x):\n",
    "    batchSize = x.shape[0]\n",
    "    self.learningIterations += batchSize\n",
    "    period = min(self.dutyCyclePeriod, self.learningIterations)\n",
    "    self.dutyCycle.mul_(period - batchSize)\n",
    "    self.dutyCycle.add_(x.gt(0).sum(dim=0, dtype=torch.float))\n",
    "    self.dutyCycle.div_(period)\n",
    "\n",
    "\n",
    "\n",
    "class KWinners2d(KWinnersBase):\n",
    "  \"\"\"\n",
    "  Applies K-Winner function to the input tensor\n",
    "  See :class:`htmresearch.frameworks.pytorch.functions.k_winners2d`\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, n, k, channels, kInferenceFactor=1.0, boostStrength=1.0,\n",
    "               boostStrengthFactor=1.0, dutyCyclePeriod=1000):\n",
    "    \"\"\"\n",
    "    :param n:\n",
    "      Number of units. Usually the output of the max pool or whichever layer\n",
    "      preceding the KWinners2d layer.\n",
    "    :type n: int\n",
    "    :param k:\n",
    "      The activity of the top k units will be allowed to remain, the rest are set\n",
    "      to zero\n",
    "    :type k: int\n",
    "    :param channels:\n",
    "      Number of channels (filters) in the convolutional layer.\n",
    "    :type channels: int\n",
    "    :param kInferenceFactor:\n",
    "      During inference (training=False) we increase k by this factor.\n",
    "    :type kInferenceFactor: float\n",
    "    :param boostStrength:\n",
    "      boost strength (0.0 implies no boosting).\n",
    "    :type boostStrength: float\n",
    "    :param boostStrengthFactor:\n",
    "      Boost strength factor to use [0..1]\n",
    "    :type boostStrengthFactor: float\n",
    "    :param dutyCyclePeriod:\n",
    "      The period used to calculate duty cycles\n",
    "    :type dutyCyclePeriod: int\n",
    "    \"\"\"\n",
    "    super(KWinners2d, self).__init__(n=n, k=k,\n",
    "                                     kInferenceFactor=kInferenceFactor,\n",
    "                                     boostStrength=boostStrength,\n",
    "                                     boostStrengthFactor=boostStrengthFactor,\n",
    "                                     dutyCyclePeriod=dutyCyclePeriod)\n",
    "\n",
    "    self.channels = channels\n",
    "    self.register_buffer(\"dutyCycle\", torch.zeros((1, channels, 1, 1)))\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Apply k-winner algorithm if k < n, otherwise default to standard RELU\n",
    "    if self.k >= self.n:\n",
    "      return F.relu(x)\n",
    "\n",
    "    if self.training:\n",
    "      k = self.k\n",
    "    else:\n",
    "      k = min(int(round(self.k * self.kInferenceFactor)), self.n)\n",
    "\n",
    "    x = k_winners2d.apply(x, self.dutyCycle, k, self.boostStrength)\n",
    "\n",
    "    if self.training:\n",
    "      self.updateDutyCycle(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "  def updateDutyCycle(self, x):\n",
    "    batchSize = x.shape[0]\n",
    "    self.learningIterations += batchSize\n",
    "\n",
    "    scaleFactor = float(x.shape[2] * x.shape[3])\n",
    "    period = min(self.dutyCyclePeriod, self.learningIterations)\n",
    "    self.dutyCycle.mul_(period - batchSize)\n",
    "    s = x.gt(0).sum(dim=(0, 2, 3), dtype=torch.float) / scaleFactor\n",
    "    self.dutyCycle.reshape(-1).add_(s)\n",
    "    self.dutyCycle.div_(period)\n",
    "\n",
    "\n",
    "  def entropy(self):\n",
    "    entropy = super(KWinners2d, self).entropy()\n",
    "    return entropy * self.n / self.channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Numenta Platform for Intelligent Computing (NuPIC)\n",
    "# Copyright (C) 2018, Numenta, Inc.  Unless you have an agreement\n",
    "# with Numenta, Inc., for a separate license for this software code, the\n",
    "# following terms and conditions apply:\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero Public License version 3 as\n",
    "# published by the Free Software Foundation.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
    "# See the GNU Affero Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero Public License\n",
    "# along with this program.  If not, see http://www.gnu.org/licenses.\n",
    "#\n",
    "# http://numenta.org/licenses/\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import Sparse as htm\n",
    "\n",
    "\n",
    "\n",
    "class SparseNet(nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               n=2000,\n",
    "               k=200,\n",
    "               outChannels=0,\n",
    "               c_k=0,\n",
    "               kernelSize=5,\n",
    "               stride=1,\n",
    "               inputSize=28*28,\n",
    "               outputSize=10,\n",
    "               kInferenceFactor=1.0,\n",
    "               weightSparsity=0.5,\n",
    "               weightSparsityCNN=0.5,\n",
    "               boostStrength=1.0,\n",
    "               boostStrengthFactor=1.0,\n",
    "               dropout=0.0,\n",
    "               useBatchNorm=True,\n",
    "               normalizeWeights=False,\n",
    "               useSoftmax=True,\n",
    "               padding=0,\n",
    "               maxPoolKernel=2):\n",
    "    \"\"\"\n",
    "    A network with one or more hidden layers, which can be a sequence of\n",
    "    k-sparse CNN followed by a sequence of k-sparse linear layer with optional\n",
    "    dropout layers in between the k-sparse linear layers.\n",
    "        [CNNSDR] x len(outChannels)\n",
    "            |\n",
    "        [Flatten]\n",
    "            |\n",
    "        [LinearSDR => Dropout] x len(n)\n",
    "            |\n",
    "        [Linear => Softmax]\n",
    "    :param n:\n",
    "      Number of units in each fully connected k-sparse linear layer.\n",
    "      Use 0 to disable the linear layer\n",
    "    :type n: int or list[int]\n",
    "    :param k:\n",
    "      Number of ON (non-zero) units per iteration in each k-sparse linear layer.\n",
    "      The sparsity of this layer will be k / n. If k >= n, the layer acts as a\n",
    "      traditional fully connected RELU layer\n",
    "    :type k: int or list[int]\n",
    "    :param outChannels:\n",
    "      Number of channels (filters) in each k-sparse convolutional layer.\n",
    "      Use 0 to disable the CNN layer\n",
    "    :type outChannels: int or list[int]\n",
    "    :param c_k:\n",
    "      Number of ON (non-zero) units per iteration in each k-sparse convolutional\n",
    "      layer. The sparsity of this layer will be c_k / c_n. If c_k >= c_n, the\n",
    "      layer acts as a traditional convolutional layer.\n",
    "    :type c_k: int or list[int]\n",
    "    :param kernelSize:\n",
    "      Kernel size to use in each k-sparse convolutional layer.\n",
    "    :type kernelSize: int or list[int]\n",
    "    :param stride:\n",
    "      Stride value to use in each k-sparse convolutional layer.\n",
    "    :type stride: int or list[int]\n",
    "    :param inputSize:\n",
    "      If the CNN layer is enable this parameter holds a tuple representing\n",
    "      (in_channels,height,width). Otherwise it will hold the total\n",
    "      dimensionality of input vector of the first linear layer. We apply\n",
    "      view(-1, inputSize) to the data before passing it to Linear layers.\n",
    "    :type inputSize: int or tuple[int,int,int]\n",
    "    :param outputSize:\n",
    "      Total dimensionality of output vector\n",
    "    :type outputSize: int\n",
    "    :param kInferenceFactor:\n",
    "      During inference (training=False) we increase k by this factor.\n",
    "    :type kInferenceFactor: float\n",
    "    :param weightSparsity:\n",
    "      Pct of weights that are allowed to be non-zero in each linear layer.\n",
    "    :type weightSparsity: float or list[float]\n",
    "    :param weightSparsityCNN:\n",
    "      Pct of weights that are allowed to be non-zero in each convolutional layer.\n",
    "    :type weightSparsityCNN: float or list[float]\n",
    "    :param boostStrength:\n",
    "      boost strength (0.0 implies no boosting).\n",
    "    :type boostStrength: float\n",
    "    :param boostStrengthFactor:\n",
    "      boost strength is multiplied by this factor after each epoch.\n",
    "      A value < 1.0 will decrement it every epoch.\n",
    "    :type boostStrengthFactor: float\n",
    "    :param dropout:\n",
    "      dropout probability used to train the second and subsequent layers.\n",
    "      A value 0.0 implies no dropout\n",
    "    :type dropout: float\n",
    "    :param useBatchNorm:\n",
    "      If True, applies batchNorm for each layer.\n",
    "    :type useBatchNorm: bool\n",
    "    :param normalizeWeights:\n",
    "      If True, each LinearSDR layer will have its weights normalized to the\n",
    "      number of non-zeros instead of the whole input size\n",
    "    :type normalizeWeights: bool\n",
    "    :param useSoftmax:\n",
    "      If True, use soft max to compute probabilities\n",
    "    :type useSoftmax: bool\n",
    "    :param padding:\n",
    "        cnn layer Zero-padding added to both sides of the input\n",
    "    :type padding: int\n",
    "    :param maxPoolKernel:\n",
    "      The size of the window to take a max over\n",
    "    :type maxPoolKernel: int\n",
    "    \"\"\"\n",
    "    super(SparseNet, self).__init__()\n",
    "\n",
    "\n",
    "    # Validate CNN sdr params\n",
    "    if isinstance(inputSize, collections.Sequence):\n",
    "      assert inputSize[1] == inputSize[2], \"sparseCNN only supports square images\"\n",
    "\n",
    "    if type(outChannels) is not list:\n",
    "      outChannels = [outChannels]\n",
    "    if type(c_k) is not list:\n",
    "      c_k = [c_k] * len(outChannels)\n",
    "    assert(len(outChannels) == len(c_k))\n",
    "    if type(kernelSize) is not list:\n",
    "      kernelSize = [kernelSize] * len(outChannels)\n",
    "    assert(len(outChannels) == len(kernelSize))\n",
    "    if type(stride) is not list:\n",
    "      stride = [stride] * len(outChannels)\n",
    "    assert(len(outChannels) == len(stride))\n",
    "    if type(padding) is not list:\n",
    "      padding = [padding] * len(outChannels)\n",
    "    assert(len(outChannels) == len(padding))\n",
    "    if type(weightSparsityCNN) is not list:\n",
    "      weightSparsityCNN = [weightSparsityCNN] * len(outChannels)\n",
    "    assert(len(outChannels) == len(weightSparsityCNN))\n",
    "    for i in range(len(outChannels)):\n",
    "      assert (weightSparsityCNN[i] >= 0)\n",
    "\n",
    "    # Validate linear sdr params\n",
    "    if type(n) is not list:\n",
    "      n = [n]\n",
    "    if type(k) is not list:\n",
    "      k = [k] * len(n)\n",
    "    assert(len(n) == len(k))\n",
    "    for i in range(len(n)):\n",
    "      assert(k[i] <= n[i])\n",
    "    if type(weightSparsity) is not list:\n",
    "      weightSparsity = [weightSparsity] * len(n)\n",
    "    assert(len(n) == len(weightSparsity))\n",
    "    for i in range(len(n)):\n",
    "      assert (weightSparsity[i] >= 0)\n",
    "\n",
    "    self.k = k\n",
    "    self.kInferenceFactor = kInferenceFactor\n",
    "    self.n = n\n",
    "    self.outChannels = outChannels\n",
    "    self.c_k = c_k\n",
    "    self.inputSize = inputSize\n",
    "    self.weightSparsity = weightSparsity   # Pct of weights that are non-zero\n",
    "    self.boostStrengthFactor = boostStrengthFactor\n",
    "    self.boostStrength = boostStrength\n",
    "    self.kernelSize = kernelSize\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.learningIterations = 0\n",
    "\n",
    "\n",
    "    inputFeatures = inputSize\n",
    "    outputLength = inputFeatures\n",
    "    cnnSdr = nn.Sequential()\n",
    "    # CNN Layers\n",
    "    for i in range(len(outChannels)):\n",
    "      if outChannels[i] != 0:\n",
    "        inChannels, h, w = inputFeatures\n",
    "        cnn = nn.Conv2d(in_channels=inChannels,\n",
    "                        out_channels=outChannels[i],\n",
    "                        kernel_size=kernelSize[i],\n",
    "                        padding=padding[i],\n",
    "                        stride=stride[i])\n",
    "\n",
    "        if 0 < weightSparsityCNN[i] < 1:\n",
    "          sparseCNN = htm.SparseWeights2d(cnn, weightSparsityCNN[i])\n",
    "          cnnSdr.add_module(\"cnnSdr{}_cnn\".format(i + 1), sparseCNN)\n",
    "        else:\n",
    "          cnnSdr.add_module(\"cnnSdr{}_cnn\".format(i + 1), cnn)\n",
    "\n",
    "        # Batch Norm\n",
    "        if useBatchNorm:\n",
    "          bn = nn.BatchNorm2d(outChannels[i], affine=False)\n",
    "          cnnSdr.add_module(\"cnnSdr{}_bn\".format(i + 1), bn)\n",
    "\n",
    "        # Max pool\n",
    "        maxpool = nn.MaxPool2d(kernel_size=maxPoolKernel)\n",
    "        cnnSdr.add_module(\"cnnSdr{}_maxpool\".format(i + 1), maxpool)\n",
    "\n",
    "        wout = (w + 2 * padding[i] - kernelSize[i]) // stride[i] + 1\n",
    "        maxpoolWidth = wout // 2\n",
    "        outputLength = maxpoolWidth * maxpoolWidth * outChannels[i]\n",
    "        if 0 < c_k[i] < outputLength:\n",
    "          kwinner = KWinners2d(n=outputLength, k=c_k[i],\n",
    "                                   channels=outChannels[i],\n",
    "                                   kInferenceFactor=kInferenceFactor,\n",
    "                                   boostStrength=boostStrength,\n",
    "                                   boostStrengthFactor=boostStrengthFactor)\n",
    "          cnnSdr.add_module(\"cnnSdr{}_kwinner\".format(i + 1), kwinner)\n",
    "        else:\n",
    "          cnnSdr.add_module(\"cnnSdr{}_relu\".format(i + 1), nn.ReLU())\n",
    "\n",
    "        # Feed this layer output into next layer input\n",
    "        inputFeatures = (outChannels[i], maxpoolWidth, maxpoolWidth)\n",
    "\n",
    "    if len(cnnSdr) > 0:\n",
    "      inputFeatures = outputLength\n",
    "      self.cnnSdr = cnnSdr\n",
    "    else:\n",
    "      self.cnnSdr = None\n",
    "\n",
    "    # Flatten input before passing to linear layers\n",
    "    self.flatten = torch.nn.Flatten()\n",
    "\n",
    "    # Linear layers\n",
    "    self.linearSdr = nn.Sequential()\n",
    "\n",
    "    for i in range(len(n)):\n",
    "      if n[i] != 0:\n",
    "        linear = nn.Linear(inputFeatures, n[i])\n",
    "        if 0 < weightSparsity[i] < 1:\n",
    "          linear = htm.SparseWeights(linear, weightSparsity=weightSparsity[i])\n",
    "          if normalizeWeights:\n",
    "            linear.apply(htm.normalizeSparseWeights)\n",
    "        self.linearSdr.add_module(\"linearSdr{}\".format(i + 1), linear)\n",
    "\n",
    "        if useBatchNorm:\n",
    "          self.linearSdr.add_module(\"linearSdr{}_bn\".format(i + 1),\n",
    "                                    nn.BatchNorm1d(n[i], affine=False))\n",
    "\n",
    "        if dropout > 0.0:\n",
    "          self.linearSdr.add_module(\"linearSdr{}_dropout\".format(i + 1),\n",
    "                                    nn.Dropout(dropout))\n",
    "\n",
    "        if 0 < k[i] < n[i]:\n",
    "          kwinner = KWinners(n=n[i], k=k[i],\n",
    "                                 kInferenceFactor=kInferenceFactor,\n",
    "                                 boostStrength=boostStrength,\n",
    "                                 boostStrengthFactor=boostStrengthFactor)\n",
    "          self.linearSdr.add_module(\"linearSdr{}_kwinner\".format(i + 1), kwinner)\n",
    "        else:\n",
    "          self.linearSdr.add_module(\"linearSdr{}_relu\".format(i + 1), nn.ReLU())\n",
    "\n",
    "        # Feed this layer output into next layer input\n",
    "        inputFeatures = n[i]\n",
    "\n",
    "    # Add one fully connected layer after all hidden layers\n",
    "    self.fc = nn.Linear(inputFeatures, outputSize)\n",
    "\n",
    "    # Use softmax to compute probabilities\n",
    "    if useSoftmax:\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "    else:\n",
    "      self.softmax = None\n",
    "\n",
    "\n",
    "  def postEpoch(self):\n",
    "    self.apply(htm.updateBoostStrength)\n",
    "    self.apply(htm.rezeroWeights)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    if self.cnnSdr is not None:\n",
    "      x = self.cnnSdr(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.linearSdr(x)\n",
    "    x = self.fc(x)\n",
    "\n",
    "    if self.softmax is not None:\n",
    "      x = self.softmax(x)\n",
    "\n",
    "    if self.training:\n",
    "      batchSize = x.shape[0]\n",
    "      self.learningIterations += batchSize\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "  def getLearningIterations(self):\n",
    "    return self.learningIterations\n",
    "\n",
    "  def maxEntropy(self):\n",
    "    entropy = 0\n",
    "    for module in self.modules():\n",
    "      if module == self:\n",
    "        continue\n",
    "      if hasattr(module, \"maxEntropy\"):\n",
    "        entropy += module.maxEntropy()\n",
    "\n",
    "    return entropy\n",
    "\n",
    "  def entropy(self):\n",
    "    \"\"\"\n",
    "    Returns the current entropy\n",
    "    \"\"\"\n",
    "    entropy = 0\n",
    "    for module in self.modules():\n",
    "      if module == self:\n",
    "        continue\n",
    "      if hasattr(module, \"entropy\"):\n",
    "        entropy += module.entropy()\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "  def pruneWeights(self, minWeight):\n",
    "    \"\"\"\n",
    "    Prune all the weights whose absolute magnitude is less than minWeight\n",
    "    :param minWeight: min weight to prune. If zero then no pruning\n",
    "    :type minWeight: float\n",
    "    \"\"\"\n",
    "    if minWeight == 0.0:\n",
    "      return\n",
    "\n",
    "    # Collect all weights\n",
    "    weights = [v for k, v in self.named_parameters() if 'weight' in k]\n",
    "    for w in weights:\n",
    "      # Filter weights above threshold\n",
    "      mask = torch.ge(torch.abs(w.data), minWeight)\n",
    "      # Zero other weights\n",
    "      w.data.mul_(mask.type(torch.float32))\n",
    "\n",
    "  def pruneDutycycles(self, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Prune all the units with dutycycles whose absolute magnitude is less than\n",
    "    the given threshold\n",
    "    :param threshold: min threshold to prune. If less than zero then no pruning\n",
    "    :type threshold: float\n",
    "    \"\"\"\n",
    "    if threshold < 0.0:\n",
    "      return\n",
    "\n",
    "    # Collect all layers with 'dutyCycle'\n",
    "    for m in self.modules():\n",
    "      if m == self:\n",
    "        continue\n",
    "      if hasattr(m, 'pruneDutycycles'):\n",
    "        m.pruneDutycycles(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseNet(\n",
       "  (cnnSdr): Sequential(\n",
       "    (cnnSdr1_cnn): SparseWeights2d(\n",
       "      (module): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    )\n",
       "    (cnnSdr1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (cnnSdr1_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (cnnSdr1_kwinner): KWinners2d()\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linearSdr): Sequential(\n",
       "    (linearSdr1): SparseWeights(\n",
       "      (module): Linear(in_features=3200, out_features=2000, bias=True)\n",
       "    )\n",
       "    (linearSdr1_bn): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (linearSdr1_kwinner): KWinners()\n",
       "  )\n",
       "  (fc): Linear(in_features=2000, out_features=10, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SparseNet(outChannels=[32], inputSize=(1,24,24), c_k=25)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4544, -1.6328])\n",
      "tensor([0.5757, 0.2279])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Hardsigmoid()\n",
    "input = torch.randn(2)\n",
    "output = m(input)\n",
    "\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseWeights2d(\n",
       "  (module): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = net.cnnSdr[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zeroWts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,\n",
       "         7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
       "        13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19,\n",
       "        19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22,\n",
       "        22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25,\n",
       "        25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "        27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28,\n",
       "        28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31,\n",
       "        31, 31, 31, 31, 31, 31])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zeroWts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 5, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.module.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "384/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fed6d6cd0ea97ce3b2d4e99f7713523d71a847210ef7afa8b6d15b0ad5dcd7d5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('DeepLearning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
